{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKyc6sv7mHBLU9awgzhHwF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharvaniyadav/Assignment-1-ECE1513-Intro-To-ML/blob/main/Assignment_1_ECE1513_IntroToML_SharvaniYadav.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "X2AntL4JgzRS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "828ac687-4172-4f42-de50-b8ec8244c9ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== DATASET LOADED ===\n",
            "Shape of X (rows, columns): (569, 30)\n",
            "First row of raw data (before standardization):\n",
            "[1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01\n",
            " 1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02\n",
            " 6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01\n",
            " 1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01\n",
            " 4.601e-01 1.189e-01]\n",
            "\n",
            "=== AFTER STANDARDIZATION ===\n",
            "First row after standardization (values were now near 0):\n",
            "[ 1.097 -2.073  1.27   0.984  1.568  3.284  2.653  2.532  2.218  2.256\n",
            "  2.49  -0.565  2.833  2.488 -0.214  1.317  0.724  0.661  1.149  0.907\n",
            "  1.887 -1.359  2.304  2.001  1.308  2.617  2.11   2.296  2.751  1.937]\n",
            "\n",
            "Mean of first 5 features: [-0. -0. -0. -0.  0.]\n",
            "Std of first 5 features: [1. 1. 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "# Course: ECE1513 Introduction to Machine Learning (Fall 2025)\n",
        "# Assignment 1\n",
        "# Student Name: Sharvani Yadav\n",
        "# Student Number: 1008289870\n",
        "\n",
        "# Part 1: Clustering with k-means\n",
        "# This coded written below for Part 1 implements k-means clustering from scratch using only NumPy.\n",
        "# It uses an initialization method iscussed in class (sampled from the dataset) for reproducibility.\n",
        "# It will run the algorithm for k = 2..7, computes distortion, and plots an elbow curve.\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------\n",
        "# Part 1.1: Implemented k-means using a single initialization method (sampling points from dataset)\n",
        "# -------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Import needed libraries\n",
        "# numpy: for math with arrays\n",
        "# sklearn.datasets: to load the Breast Cancer dataset\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 1: Loaded the dataset\n",
        "# -----------------------------\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Loading the UCI ML Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Only needed the features (X) for clustering, not the labels\n",
        "X = data.data.astype(float)\n",
        "\n",
        "print(\"=== DATASET LOADED ===\")\n",
        "print(\"Shape of X (rows, columns):\", X.shape)   # should be 569 rows Ã— 30 features\n",
        "print(\"First row of raw data (before standardization):\")\n",
        "print(X[0])  # just to see the numbers before I change anything\n",
        "print()\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 2: Made all features the same scale (standardized)\n",
        "# -----------------------------\n",
        "\n",
        "# Imported NumPy for math operations\n",
        "import numpy as np\n",
        "\n",
        "# Found the average (mean) value for each feature\n",
        "feature_means = X.mean(axis=0)\n",
        "\n",
        "# Found how spread out (standard deviation) each feature was\n",
        "feature_stds = X.std(axis=0, ddof=0)\n",
        "\n",
        "# Replaced any feature with zero spread with 1 to avoid dividing by zero\n",
        "feature_stds[feature_stds == 0] = 1.0\n",
        "\n",
        "# Created a new dataset where each feature had mean 0 and spread ~1\n",
        "# This was done by subtracting the mean and dividing by the standard deviation\n",
        "Xs = (X - feature_means) / feature_stds\n",
        "\n",
        "print(\"=== AFTER STANDARDIZATION ===\")\n",
        "print(\"First row after standardization (values were now near 0):\")\n",
        "print(np.round(Xs[0], 3))  # Rounded to make it easier to read\n",
        "\n",
        "# Checked that the first few features had mean close to 0 and std close to 1\n",
        "print(\"\\nMean of first 5 features:\", np.round(Xs[:, :5].mean(axis=0), 3))\n",
        "print(\"Std of first 5 features:\", np.round(Xs[:, :5].std(axis=0, ddof=0), 3))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------\n",
        "# STEP 3: Made the small functions needed for k-means\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# Picked k starting points from the data to use as centroids\n",
        "# Used a random seed so the results stayed the same every time the code ran\n",
        "# (helpful so results match if the notebook is run again)\n",
        "def init_centroids_from_data(X, k, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = rng.choice(X.shape[0], size=k, replace=False)\n",
        "    return X[idx].copy()\n",
        "\n",
        "# Found which centroid each data point was closest to\n",
        "# First calculated the difference between each point and each centroid\n",
        "# Then calculated the squared distance and picked the smallest one\n",
        "# (squared distance was used because it's simpler and still works correctly)\n",
        "def assign_labels(X, centroids):\n",
        "    diff = X[:, None, :] - centroids[None, :, :]   # difference between points and centroids\n",
        "    distances = np.sum(diff ** 2, axis=2)          # squared distances\n",
        "    return np.argmin(distances, axis=1)            # index of closest centroid\n",
        "\n",
        "# Moved each centroid to the middle (average) of all the points in its group\n",
        "# If a group had no points, used the overall average of the data instead\n",
        "# (this felt like the safest fallback so the algorithm wouldn't crash)\n",
        "def recompute_centroids(X, labels, k):\n",
        "    d = X.shape[1]\n",
        "    new_centroids = np.empty((k, d), dtype=X.dtype)\n",
        "    global_mean = X.mean(axis=0)\n",
        "    for j in range(k):\n",
        "        pts = X[labels == j]\n",
        "        if pts.size > 0:\n",
        "            new_centroids[j] = pts.mean(axis=0)\n",
        "        else:\n",
        "            new_centroids[j] = global_mean\n",
        "    return new_centroids\n"
      ],
      "metadata": {
        "id": "XJ4pKF2svGh4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# STEP 4: Put the pieces together (full k-means)\n",
        "# -----------------------------\n",
        "\n",
        "# This function ran the whole k-means process.\n",
        "# Simple idea: started with some centroids, then repeated:\n",
        "#   1) gave each point the label of the closest centroid\n",
        "#   2) moved each centroid to the average of its labeled points\n",
        "# kept going until the centroids barely moved anymore\n",
        "def kmeans(X, k, *, max_iter=300, tol=1e-4, seed=42):\n",
        "    # Started with k centroids picked from the data (see Step 3)\n",
        "    centroids = init_centroids_from_data(X, k, seed)\n",
        "\n",
        "    # Looped up to max_iter times in case things took a while to settle\n",
        "    for i in range(max_iter):\n",
        "        # Step 1: Gave each point the label of its nearest centroid\n",
        "        labels = assign_labels(X, centroids)\n",
        "\n",
        "        # Step 2: Recomputed the centroids based on those labels\n",
        "        new_centroids = recompute_centroids(X, labels, k)\n",
        "\n",
        "        # Calculated how far the centroids moved since last time\n",
        "        diff = new_centroids - centroids\n",
        "        shift = np.linalg.norm(diff)\n",
        "\n",
        "        # Updated centroids for the next round\n",
        "        centroids = new_centroids\n",
        "\n",
        "        # Added a check so the function could stop early\n",
        "        # if the movement was smaller than the tolerance (means it settled)\n",
        "        if shift < tol:\n",
        "            break\n",
        "\n",
        "    # Returned the final centroids and the label for every point\n",
        "    return centroids, labels"
      ],
      "metadata": {
        "id": "cdoOqDFk5qZ8"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}